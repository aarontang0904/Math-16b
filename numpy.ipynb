{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to `numpy`\n",
    "\n",
    "This week we will learn how to use the `numpy` library to quickly perform all the familiar computations from our linear algebra class. There is, however, a complication: we will be making use of \"numerical computation,\" i.e., computation with precision limited by the inherent inaccuracy of floating point arithmetic. The inherent inaccuracy of the computations we perform will require us to be careful in how we use the `numpy` library and interpret its results.\n",
    "\n",
    "You can find reference resources here:\n",
    "- [Library reference](https://docs.scipy.org/doc/numpy/reference/)\n",
    "- [Quickstart tutorial](https://www.numpy.org/devdocs/user/quickstart.html)\n",
    "\n",
    "We'll start by importing the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrays\n",
    "\n",
    "The main `numpy` object is the `ndarray`, a very general structure that can represent both vectors $\\vec{x} \\in \\mathbb{R}^n$ and matrices $M \\in \\mathbb{R}^{m \\times n}$. In fact, the `ndarray` represents a much more general object called a \"multidimensional array\" or \"tensor.\" For now, we'll focus on the more familiar vectors and matrices.\n",
    "\n",
    "An `ndarray` can be constructed explicitly by passing its entries to the `array` method. For a vector, we specify a list of entries. For a matrix, we specify a list of rows (of the same length), with each row given by a list of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([10, 15, 20]) # a 3-vector\n",
    "M = np.array([[0,0,0], # a 2x3 matrix\n",
    "              [3,2.0,3]])\n",
    "print(x)\n",
    "print() # blank line for separation\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created a vector $\\vec{x} \\in \\mathbb{R}^3$ using list of 3 entries, and a matrix $M \\in \\mathbb{R}^{2 \\times 3}$ with a list of 2 lists, where each of the 2 lists contains the 3 entries of one of the rows of $M$.\n",
    "You might notice that the $3$-vector `x` we defined is displayed as a row vector rather than a column vector. Usually, this distinction will not matter. But if you really want a column vector, you can instead make an $n\\times 1$ matrix.\n",
    "\n",
    "The two most basic attributes of an `ndarray` are its shape and its datatype. The shape refers to its dimensions - a single integer for an ordinary vector, a pair of integers for a matrix - and the datatype is the numerical type of its entries, for example integer or floating point with a given precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape and datatype of x:\", x.shape, x.dtype)\n",
    "print(\"shape and datatype of M:\", M.shape, M.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of integers used to describe the shape is variously called the number of *axes* of the `ndarray`, or its *dimension* or *rank*. These latter two terms can be a bit confusing, since they conflict with the usual notion of dimension and rank from linear algebra. For example, `x` is considered a 1-dimensional (or rank 1) `ndarray` object, because `len(x.shape)==1`. By contrast, in linear algebra, we might have considered `x` to be a 3-dimensional vector. Similarly, `M` is considered a 2-dimensional or rank-2 `ndarray` object. But its usual linear-algebraic rank is 1.\n",
    "\n",
    "In an attempt to avoid this confusion, we will usually use the phrase *number of axes* to describe the quantity `len(x.shape)` for `ndarray` objects `x`. But you should be aware that the other terms, dimension and rank, are also commonly used, in case you read online resources to get help.\n",
    "\n",
    "We see that `x` has the `int64` datatype, i.e., its entries are integers stored using 64 bits of memory. The `array` method used to construct `x` inferred that this was the appropriate type since the entries of the list used to construct it were all of the Python integer type (64 bits is the default precision). On the other hand, one of the entries of `M` was a Python float, so the entire matrix is stored in floating point format.\n",
    "\n",
    "Note that if you want to extract a particular component of the shape of a matrix - like the number of rows or number of columns - you can index into the shape (which is a Python tuple, and behaves like a read-only list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"M has this many columns\", M.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to construct a `numpy` array with a given shape (and perhaps a given datatype) without having to individually specify all its entries. For example, we may wish to fill in the entries of the array one-by-one later on. In these cases, there are two handy array creation routines provided by `numpy` for creating an array filled with either zeros or ones:\n",
    "- `zeros(shape)` --- creates a `numpy ndarray` with the specified shape and all entries equal to zero\n",
    "- `ones(shape)` --- the same, but all entries equal to one\n",
    "(Both methods use the `float64` datatype by default, but the datatype can be specified with the optional `dtype` argument (e.g., `dtype=np.int32`))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros((100,100)) # creates a 100x100 zero matrix (with floating point entries)\n",
    "b = np.ones(9, dtype=np.int64) # creates a 9-vector with entries all equal to 1 (as integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mention one more convenience method for defining identity matrices. To make the (square) $n \\times n$ identity matrix, you can use the method `eye(n)` (which also takes an optional `dtype` argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_matrix = np.eye(5) # 5x5 identity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetic\n",
    "\n",
    "Arithmetic operations in `numpy` are usually applied **elementwise**, and produce the result in a new `ndarray`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do some arithmetic on a 2x2 matrix\n",
    "M = np.array([[2,1],\n",
    "              [1,2]])\n",
    "print(\"M =\")\n",
    "print(M)\n",
    "print()\n",
    "\n",
    "# Addition is elementwise, just like normal matrix arithmetic:\n",
    "print(\"M+M =\")\n",
    "print(M+M)\n",
    "print()\n",
    "\n",
    "# But multiplication is also elementwise, NOT like normal matrix arithmetic:\n",
    "print(\"M*M =\")\n",
    "print(M*M)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the usual matrix multiplication from linear algebra, you should instead use the `@` operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M@M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is very important, and a common source of errors:** the `*` operation is **not** matrix-matrix or matrix-vector multiplication as defined in linear algebra. It is entrywise multiplication, which is not an operation we usually consider in linear algebra.\n",
    "\n",
    "There is however one case when entrywise multiplication was considered in linear algebra class: multiplication of a vector or matrix by a scalar. This behaves as expected in `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(3*M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But `numpy` will also let you *add* a scalar to a matrix! This is not permitted in usual linear algebra, but is perfectly acceptable in `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M+0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even perform arithmetic between vectors in $\\mathbb{R}^n$ (i.e., `ndarray`s of shape `(n,)`) and matrices in $\\mathbb{R}^{m\\times n}$ (i.e., `ndarray`s of shape `(m,n)`). In this case, the arithmetic is performed row-wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([[1,2,3],\n",
    "              [3,2,1]])\n",
    "x = np.array([2,1,0])\n",
    "print(\"M+x =\")\n",
    "print(M+x)\n",
    "print()\n",
    "print(\"M*x =\")\n",
    "print(M*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please take a moment to appreciate that **numpy arithmetic is not the same as linear algebra arithmetic.** In linear algebra, the above operations are not permitted, and do not make any sense. In numpy, they is perfectly acceptable.\n",
    "\n",
    "You will need to pay **careful attention** to the shape and meaning of your numpy arrays in order to avoid accidently doing the wrong sort of arithmetic.\n",
    "\n",
    "What if we want the usual matrix-vector multiplication from linear algebra? We need to again use the `@` operator instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M@x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the `@` operator to compute inner products of two vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x@x) # Inner (dot) product of x with itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we mention that you can find the transpose of a matrix `M` by writing `M.T`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "Indexing into an `ndarray` with one axis works just like a Python list. Try predicting what the following code will do before executing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(range(10))\n",
    "print(\"x =\", x)\n",
    "print(\"x[3] =\", x[3])\n",
    "\n",
    "x[4] *= -1\n",
    "print(\"now, x =\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an array with 2 axes (a matrix), things are only slightly more complicated. You can use a comma to separate indices for rows and columns. Try predicting what the output of the following block of code will be before executing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.zeros((3,4))\n",
    "M[0,0] = 3 # assign 3 to the (0,0) entry\n",
    "M[0,1] = 2\n",
    "M[1,0] = 4\n",
    "M[2,2] = 1\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't use a comma when indexing a 2-axis array, you'll get entire rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"first row:\", M[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to access a particular column of matrix (2-axis array), you can use `:` as the first comma-separated index to select \"all rows\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"first column:\", M[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, to access columns of a matrix, you could take the transpose and index the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"also the first column:\", M.T[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Floating point arithmetic\n",
    "\n",
    "Before discussing linear algebra operations, its important to take some time to empahsize that floating point arithmetic is *not* the same as real arithmetic. For example, there is no \"largest\" real number, but there is a largest floating point number (which might depend on the particulars of the computer you're using). You can find the largest floating point number on your system using the `sys` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.float_info.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my system, this largest number is roughly $1.8\\times 10^{308}$. It is impossible to represent a larger number in Python using float point arithmetic (whereas there are of course larger real numbers).\n",
    "\n",
    "Another small difference between floating point arithmetic and real arithmetic is that there are three special floating point numbers that don't correspond any real number: `inf`, `-inf`, and `nan`. The first two correspond to positive and negative infinity, respectively; they sometimes appear if you perform some arithmetic and end up with a number larger than `sys.float_info.max`. The latter, `nan`, is short for \"not a number\" and appears for example if you multiply 0 by `inf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2*sys.float_info.max) # twice the largest floating point number is 'inf'\n",
    "print(0*float('inf')) # 0*inf=nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But far more important than its non-real values is the fact that **floating point arithmetic is fundamentally, unavoidably inaccurate**. *All* computation involving real numbers (as opposed to integers) needs to confront this inaccuracy.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1+0.2==0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are infinitely many real numbers. In fact, there are infinitely many real numbers just between $0$ and $1$. But your computer only has a finite amount of memory. So it is physically impossible for your computer to even represent absolutely any real number. Instead, only a finite subset of the real numbers can be represented --- these are the floating point numbers. Every other real number you'd like to express will be rounded to one of the floating point numbers.\n",
    "\n",
    "Since we can't even represent the results of arbitrary real arithmetic, there's also no point in computing that result exactly. Instead, floating point arithmetic is (relatively) fast, but slightly inaccurate as a result. This topic is discussed in greater depth in Math 122a.\n",
    "\n",
    "Here are some tips for avoiding some of the problems arising from this inaccuracy:\n",
    "- **Never** compare floating point numbers for equality. If absolutely necessary, you can check if two floating point numbers are *approximately equal* (for example using the `np.isclose` function)\n",
    "- Similarly, be careful with inequalities involving float point numbers --- you may need to allow some error tolerance\n",
    "- Consider using `int`s instead of `float`s when possible (e.g., when you have rational numbers and can multiply by a common denominator). (Integer arithmetic not involving division is perfectly accurate; but extremely large numbers can't be stored as integers.)\n",
    "- Use numpy's built-in routines for arithmetic involving many floating point numbers, rather than, e.g., iteratively performing the arithmetic using a for-loop. Numpy routines are carefully designed to minimize the accumulation of floating point errors\n",
    "\n",
    "Here are some examples of avoiding floating point inaccuracy. First, let's check for equality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(0.1+0.2==0.3) # NEVER DO THIS\n",
    "print(np.isclose(0.1+0.2, 0.3)) # Instead, check if floating point numbers are \"approximately equal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, a BAD APPROACH YOU SHOULD NEVER USE\n",
    "x=0.1\n",
    "while x <= 1.5:\n",
    "    print(x)\n",
    "    x += 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that (in addition to have some inaccuracy), we are missing the last entry (`1.5`) entirely, as a consequence of not having any error tolerance when comparing `x` and `1.5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A better approach: allow some error tolerance\n",
    "epsilon=0.01 #allow this much error in comparisons of floating point values\n",
    "x=0.1\n",
    "while x <= 1.5 + epsilon:\n",
    "    print(x)\n",
    "    x += 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better would be to avoid most of the floating point arithmetic altogether, and stick with integers as much as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,16):\n",
    "    print(x/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better still: try using built-in `numpy` functionality wherever possible. In this case, there is a builtin numpy function called `linspace` that does exactly what we want:\n",
    "[See the documentation for linspace here.](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0.1, 1.5, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful numpy routines, particularly when trying to mitigate floating point inaccuracy:\n",
    "- `np.sum(x)` --- sum all the entries in x\n",
    "- `np.prod(x)` --- compute the product of all the entries in x\n",
    "- `np.mean(x)` --- compute the mean of all the entries in x\n",
    "\n",
    "Let's see the difference between `np.sum` and trying to sum things up manually. We'll use lower-precision (32-bit) floating point numbers in order to make the difference more stark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0.1*np.ones((100000,), dtype=np.float32) # We'll sum up all the entries in the array x\n",
    "x_sum = np.zeros((), dtype=np.float32) # stores the sum of all the entries, starting at 0\n",
    "for entry in x:\n",
    "    x_sum += entry\n",
    "print(\"Result of manually summing: \", x_sum)\n",
    "\n",
    "print(\"Result of built-in numpy routine: \", np.sum(x)) # Substantially more accurate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear algebra with `numpy`\n",
    "\n",
    "We'll now learn how to do the following in `numpy`:\n",
    "- solve systems of linear equations\n",
    "- compute determinants, norms, matrix ranks, and matrix inverses\n",
    "- compute eigenvectors and eigenvectors\n",
    "\n",
    "A number of potential pitfalls will arise, mostly because floating point arithmetic is not the same as real arithmetic. We'll discuss how to navigate around these pitfalls.\n",
    "\n",
    "Most of the familiar tasks from your linear algebra class can be achieved using functions in the `numpy.linalg` module. The [documentation is available here](https://docs.scipy.org/doc/numpy/reference/routines.linalg.html).\n",
    "\n",
    "### Solving systems of linear equations\n",
    "\n",
    "One of the most essential linear algebra tasks is solving systems of linear equations. Suppose you want to solve a system of the form\n",
    "$$ Ax = b $$\n",
    "where $A \\in \\mathbb{R}^{n \\times n}$ is a known matrix, $b \\in \\mathbb{R}^n$ is a known vector, and $x \\in \\mathbb{R}^{n}$ is what we want to find. Assuming $A$ is **square** and of full rank, you can use `numpy.linalg.solve` to find $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,1],[0,1]])\n",
    "b = np.array([2,1])\n",
    "x = np.linalg.solve(A, b)\n",
    "\n",
    "print(\"A @ x =\")\n",
    "print(A @ x)\n",
    "print()\n",
    "print(\"b = \")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $A$ is not square, or not of full rank, then a solution might not exist or there might exist multiple solutions. In order to find a solution in this case, you should instead use the `numpy.linalg.lstsq` function. (Note: this function will display a warning that can be ignored, or surpressed by passing another parameter to the function; see the [documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html#numpy.linalg.lstsq).)\n",
    "\n",
    "The `lstsq` function will produce the *least squares* solution. We will talk more about least squares in a later week. The function will find $x$ minimizing the norm $\\|Ax - b\\|$.\n",
    "It returns a bunch of stuff, in addition to $x$. Specifically, it returns\n",
    "- the least squares solution $x$\n",
    "- the amount of residual error (i.e., the quantity $\\|Ax - b\\|^2$), except depending on the shape and rank of $A$ it might not be given; for now, you should probably ignore it\n",
    "- the rank of $A$ (from which you can infer the dimension of the space of solutions); often you won't need this either\n",
    "- the singular values of $A$ (which you should almost surely ignore, at this point)\n",
    "All of this will be discussed in greater detail later in the course.\n",
    "\n",
    "Since the function returns 4 different values, you'll need variables for all of them. But since you probably only care about the first value, it's recommended to use the dummy variable `_` for values you don't want to examine.\n",
    "\n",
    "We can check if the returned solution $x$ is indeed a solution (i.e., if $Ax$ is approximately $b$) by using the `np.allclose` function. (Remember, not all systems of linear equations have solutions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,1,0],[0,1,0]])\n",
    "# get least squares solution, and set the remaining returned values to a dummy variable\n",
    "x, _, _, _ = np.linalg.lstsq(A,b)\n",
    "\n",
    "if np.allclose(A@x, b):\n",
    "    # all entries of A@x are close to the corresponding entries of b\n",
    "    print(\"found solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The determinant\n",
    "\n",
    "Computing the determinant of a square matrix is straightforward and fool-proof. Of course, there will be some small amount of floating point error, so avoid e.g. checking if the determinant is exactly equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2],[3,4]])\n",
    "print(np.linalg.det(A)) # the determinant of A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The rank\n",
    "\n",
    "Computing the rank of a matrix is slightly more subtle, because it's important to be clear about what you *mean* by the rank. Since floating point arithmetic is inexact, if you produce a matrix $A$ after a sequence of floating point computations, it will usually have accumulated tiny errors in many or all of its entries, and these tiny perturbations have a strong effect on the rank of the matrix under the usual linear algebraic defintion of matrix rank for real numbers. For example, below we compute the outer product $A$ of a vector $x$. We then compute $A$ separately (as `A_real`) by using real arithmetic (e.g. with pen and paper) and directly specifying its entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0.1, 0.7, 1.1]])\n",
    "A = x.T @ x # A \n",
    "A_real = np.array([[0.01, 0.07, 0.11],\n",
    "                   [0.07, 0.49, 0.77],\n",
    "                   [0.11, 0.77, 1.21]])\n",
    "if np.allclose(A, A_real):\n",
    "    print(\"A and A_real have the same entries, except for tiny perturbations\")\n",
    "    \n",
    "print(A - A_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $A$ is an outer product, all of the rows of $A$ are scalar multiples of one another, and $A$ should have rank one (in real arithmetic). Linear-algebraically speaking, both `A` and `A_real` should have matrix rank equal to 1. And since `A` and `A_real` are (morally speaking) the same matrix (computed in two different ways), their difference should have rank 0. But using the `numpy.linalg.matrix_rank` function to compute the ranks, we instead see the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rank of A is\", np.linalg.matrix_rank(A))\n",
    "print(\"Rank of A_real is\", np.linalg.matrix_rank(A_real))\n",
    "print(\"Rank of A-A_real is\", np.linalg.matrix_rank(A-A_real))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this would not make any sense in linear algebra; the rank of the sum (or difference) of two matrices is at most the sum of the ranks, so in usual linear algebra it is impossible to subtract one rank-1 matrix from another and have the result be rank-3.\n",
    "\n",
    "All of this is caused by the fundamental problem of inaccuracy of floating point arithmetic. When computing the rank, you need to decide if numbers are zero or nonzero (e.g. to decide what the leading entries are when computing echelon form, or to decide if a linear combination of vectors is equal to zero). Using exact equality is undesirable since floating point arithmetic is inexact, so you need to make a decision about what counts as \"close enough\" to zero.\n",
    "\n",
    "The way `numpy.linalg.matrix_rank` makes this decision is by looking at the \"singular values\" of the matrix; singular values that are close to zero *compared to the largest singular value* are considered to be exactly zero, and the rank is the number of nonzero singular values. (You can learn more about singular values by taking Math 122a.) If you really need to know the rank of a matrix, using `numpy.linalg.matrix_rank` is a pretty good way to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverses\n",
    "\n",
    "Computing the inverse of a matrix is also fraught with peril, and is almost never a good idea. If you are thinking of computing the inverse of a matrix, first try looking for another solution (e.g., directly solving a system of linear equations using the functions described above.) The main difficulty with computing matrix inverses is that floating point values that \"should\" equal zero often are not exactly zero, so `numpy` will allow you to compute the inverse of matrices that are, morally speaking, not full rank. Indeed, `numpy` will happily invert many matrices for which `numpy.linalg.matrix_rank` gives an answer less than full rank. To compute an inverse (which again, I stress, is something you shouldn't do), use the `numpy.linalg.inv` function. (Or better, don't.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linalg.inv(A)) # A has rank 1, as even numpy will tell you, so we probably shouldn't be inverting it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norms\n",
    "\n",
    "Computing the norm of a vector is, by contrast, a relatively benign operation. You use the `numpy.linalg.norm` function to compute the norm $\\|x\\|$ of a vector $x$. Be careful to only use this function on *vectors* with a single axis (of any length), i.e., `numpy` arrays of shape `(n,)` for some integer `n`. There are many different notions of the norm of a vector, and there are also many different notions of the norm of a *matrix*; by default, if you pass a vector (rather than a matrix) to `numpy.linalg.norm`, you'll get the Euclidean norm of that vector (which is the norm you are familiar with from linear algebra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linalg.norm(np.array([1,1,1,1]))) # sqrt(1*1 + 1*1 + 1*1 + 1*1) = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues and eigenvectors\n",
    "\n",
    "Computing the eigenvalues of a square matrix is done using the `numpy.linalg.eig` function. The function `numpy.linalg.eig` returns both the eigenvalues, and a matrix of eigenvectors. The **columns** of the latter matrix are the eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0,1],[1,0]]) \n",
    "eigvals, eigvecs = np.linalg.eig(A)\n",
    "# eigvals is a numpy array of shape (2,), the eigenvalues of A\n",
    "# eigvecs is a numpy array of shape (2,2); the COLUMNS of this matrix are the eigenvectors of A\n",
    "\n",
    "for i, eigvalue in enumerate(eigvals):\n",
    "    # eigvalue is the i'th eigenvalue of A\n",
    "    eigvector = eigvecs[:,i] # the ith COLUMN of eigvecs is the corresponding eigenvector\n",
    "    print(np.allclose(A @ eigvector, eigvalue * eigvector)) # A times eigenvector equals eigenvalue times eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please keep in mind that because the eigenvalues are computed using floating point arithmetic, they will be inexact! In particular, it might be hard to decide whether you have the same eigenvalue with multiplicity greater than 1, or multiple distinct eigenvalues. Worse, `numpy.linalg.eig` will always return $n$ eigenvalues and a matrix of $n$ eigenvectors, for any $n\\times n$ matrix you provide, even if the matrix has fewer than $n$ (geometric) eigenvectors. (The returned matrix of eigenvectors should in that case have `numpy.linalg.matrix_rank` less than $n$.)\n",
    "\n",
    "For example, the following matrix has only one eigenvector (the eigenvalue $1$ has algebraic multiplicity 2, but geometric multiplicity only 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,1], [0,1]])\n",
    "print(\"The matrix A:\\n\", A)\n",
    "\n",
    "eigvals, eigvecs = np.linalg.eig(A)\n",
    "print(\"The eigenvalue(s):\", eigvals)\n",
    "print(\"The eigenvector(s):\\n\", eigvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the columns of the eigenvectors matrix are almost, but not exactly, multiples of one another. The columns are really giving the same eigenvector, up to a scalar multiple and some floating point inaccuracy.\n",
    "\n",
    "A final complication to keep in mind is that even if your matrix is real, some of the eigenvalues and eigenvectors may be complex, as in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0,-1], [1,0]])\n",
    "print(\"The matrix A:\\n\", A)\n",
    "\n",
    "eigvals, eigvecs = np.linalg.eig(A)\n",
    "print(\"The eigenvalue(s):\", eigvals)\n",
    "print(\"The eigenvector(s):\\n\", eigvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The square root of $-1$ is represented in Python as `j`, and can be produced with `complex('j')`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex('j') # the square root of -1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
